{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ec3ddc-15bb-459d-8944-c3237ef824b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm # Pour afficher une barre de progression\n",
    "\n",
    "# Paramètres de l'augmentation\n",
    "INPUT_CSV_PATH = \"corpus.csv\" # corpus initial\n",
    "OUTPUT_CSV_PATH = \"corpus_augmente_paraphrases.csv\"\n",
    "MODEL_NAME = \"google/mt5-base\" # modèle T5 multilingue\n",
    "NUM_PARAPHRASES_PER_ARTICLE = 1 # Nombre de versions paraphrasées à générer par article initial\n",
    "MAX_SENTENCES_PER_ARTICLE = 5 # Limite le nombre de phrases à paraphraser par article pour gérer la longueur\n",
    "SENTENCE_SPLITTER = \".\" # Le caractère à utiliser pour diviser l'article en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac063b1-b177-48b4-b603-bf07b6d3e8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle et du tokenizer : google/mt5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catalinciomne/anaconda3/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle déplacé vers : cpu\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle et le tokenizer\n",
    "print(f\"Chargement du modèle et du tokenizer : {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Détecter l'appareil (GPU ou CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Modèle déplacé vers : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9578677f-eb73-40e8-8a38-afefa14754c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initial chargé depuis corpus.csv. Nombre d'articles : 40\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour générer des paraphrases\n",
    "def generate_paraphrases(text, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Génère des paraphrases pour un texte donné.\n",
    "    \"\"\"\n",
    "    input_text = text\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # Génération des paraphrases\n",
    "    # num_beams pour la recherche en faisceau (meilleure qualité)\n",
    "    # no_repeat_ngram_size pour éviter la répétition de n-grammes\n",
    "    # max_length pour limiter la longueur des paraphrases\n",
    "    # early_stopping=True pour arrêter la génération quand tous les beams sont terminés\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        max_length=128,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    paraphrases = []\n",
    "    for output in outputs:\n",
    "        decoded_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # Supprimer les tokens <extra_id_X>\n",
    "        import re\n",
    "        cleaned_text = re.sub(r'<extra_id_\\d+>', '', decoded_text).strip()\n",
    "        paraphrases.append(cleaned_text)\n",
    "\n",
    "    return paraphrases\n",
    "\n",
    "# Charger le dataset initial\n",
    "try:\n",
    "    df_initial = pd.read_csv(INPUT_CSV_PATH)\n",
    "    print(f\"Dataset initial chargé depuis {INPUT_CSV_PATH}. Nombre d'articles : {len(df_initial)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur : Le fichier {INPUT_CSV_PATH} n'a pas été trouvé.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aec8415-a87e-462d-9101-9587370af519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la génération de 1 paraphrases par article initial...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a5d60574ed4634bf0ccdb18bd35dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmentation des articles:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Créer une liste pour stocker les articles augmentés\n",
    "augmented_articles = []\n",
    "\n",
    "# Ajouter les articles initiaux au dataset augmenté\n",
    "augmented_articles.extend(df_initial.to_dict('records'))\n",
    "\n",
    "# Procéder à l'augmentation\n",
    "print(f\"Début de la génération de {NUM_PARAPHRASES_PER_ARTICLE} paraphrases par article initial...\")\n",
    "for index, row in tqdm(df_initial.iterrows(), total=len(df_initial), desc=\"Augmentation des articles\"):\n",
    "    original_text = row['article']\n",
    "    label = row['catégorie']\n",
    "\n",
    "    # Diviser l'article en phrases\n",
    "    sentences = [s.strip() for s in original_text.split(SENTENCE_SPLITTER) if s.strip()]\n",
    "    \n",
    "    # Limiter le nombre de phrases à paraphraser pour des raisons de performance et de cohérence\n",
    "    sentences_to_paraphrase = sentences[:MAX_SENTENCES_PER_ARTICLE]\n",
    "\n",
    "    generated_texts_for_article = []\n",
    "\n",
    "    for _ in range(NUM_PARAPHRASES_PER_ARTICLE):\n",
    "        paraphrased_sentences = []\n",
    "        for sentence in sentences_to_paraphrase:\n",
    "            # Générer une seule paraphrase par phrase pour éviter un mélange trop grand\n",
    "            paraphrase = generate_paraphrases(sentence, num_return_sequences=1)\n",
    "            if paraphrase:\n",
    "                paraphrased_sentences.append(paraphrase[0])\n",
    "            else:\n",
    "                paraphrased_sentences.append(sentence) # Garder l'original si pas de paraphrase\n",
    "\n",
    "        # Reconstruire l'article paraphrasé\n",
    "        # Si l'article original a plus de phrases que MAX_SENTENCES_PER_ARTICLE,\n",
    "        # on concatène les paraphrases avec le reste des phrases originales.\n",
    "        reconstructed_article = (SENTENCE_SPLITTER + \" \").join(paraphrased_sentences)\n",
    "        if len(sentences) > MAX_SENTENCES_PER_ARTICLE:\n",
    "            reconstructed_article += (SENTENCE_SPLITTER + \" \").join(sentences[MAX_SENTENCES_PER_ARTICLE:])\n",
    "        \n",
    "        generated_texts_for_article.append(reconstructed_article)\n",
    "\n",
    "    # Ajouter les articles paraphrasés à la liste\n",
    "    for gen_text in generated_texts_for_article:\n",
    "        augmented_articles.append({\"article\": gen_text, \"catégorie\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc02533-1149-464c-9260-31f0fa02a708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aperçu du dataset augmenté ---\n",
      "                                             article catégorie  label_id\n",
      "0  Festival de Cannes 2025 : Denzel Washington re...   culture         0\n",
      "1  Eurovision 2025 : l’Autriche remporte le conco...   culture         0\n",
      "2  Werenoi, rappeur numéro un des ventes d’albums...   culture         0\n",
      "3  Flammes 2024 : Tiakola rafle la mise avec quat...   culture         0\n",
      "4  « Watch You Burn », l’art brûlot de Mathias Ki...   culture         0\n",
      "\n",
      "Nombre total d'articles après augmentation : 80\n",
      "Labels numériques créés : {'culture': 0, 'sport': 1}\n",
      "Dataset augmenté sauvegardé sous : corpus_augmente_paraphrases.csv\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le dataset augmenté\n",
    "df_augmented = pd.DataFrame(augmented_articles)\n",
    "\n",
    "# Convertir les labels textuels en numériques si nécessaire (par exemple 'culture' -> 0, 'sport' -> 1)\n",
    "unique_labels = df_augmented['catégorie'].unique()\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "df_augmented['label_id'] = df_augmented['catégorie'].map(label_to_id)\n",
    "\n",
    "print(\"\\n--- Aperçu du dataset augmenté ---\")\n",
    "print(df_augmented.head())\n",
    "print(f\"\\nNombre total d'articles après augmentation : {len(df_augmented)}\")\n",
    "print(f\"Labels numériques créés : {label_to_id}\")\n",
    "\n",
    "df_augmented.to_csv(OUTPUT_CSV_PATH, index=False, encoding=\"utf-8\")\n",
    "print(f\"Dataset augmenté sauvegardé sous : {OUTPUT_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ffdc-4c8e-41db-bc6c-842bc3f9e83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
